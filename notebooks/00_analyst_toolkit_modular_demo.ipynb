{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39b26b13",
   "metadata": {},
   "source": [
    "# 🧪 Analyst Toolkit Tutorial: Full Data Pipeline with Penguins\n",
    "\n",
    "This interactive notebook demonstrates the complete analyst pipeline using a synthetic **Palmer Penguins** dataset generated from the [`penguin_research_toolkit` repository](https://github.com/G-Schumacher44/penguin_research_toolkit).\n",
    "\n",
    "Each step in the pipeline is modular, YAML-configurable, and produces exports, plots, and certification-ready reports.\n",
    "\n",
    "This toolkit is packaged using **TOML (`pyproject.toml`)** and can be run via script or notebook.\n",
    "\n",
    "\n",
    "### 🧰 Toolkit Architecture: 3-Way Modular Design\n",
    "\n",
    "This pipeline is built around a flexible ETL framework with three usage modes:\n",
    "\n",
    "- 📓 **Notebook Mode**\n",
    "  - Run individual modules or the full pipeline interactively\n",
    "  - Supports HTML dashboards, widgets, and live previews\n",
    "  - Ideal for iterative exploration, first-pass audits, and QA workflows\n",
    "\n",
    "- 🧵 **CLI Mode**\n",
    "  - Execute the full pipeline using `run_toolkit_pipeline.py`\n",
    "  - Controlled via a master YAML config\n",
    "  - Exports all reports, checkpoints, and logs to disk\n",
    "\n",
    "- 🧪 **Hybrid Mode**\n",
    "  - Develop in notebooks, deploy via scripts\n",
    "  - Reuse the same configs across testing and production\n",
    "\n",
    "The toolkit handles essential data cleaning and transformation tasks, enabling analysts to focus on:\n",
    "- Exploratory Data Analysis (EDA)\n",
    "- Investigating anomalies and data quality issues\n",
    "- Extracting actionable insights from certified data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b18c348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📁 Load Configuration and Set Execution Context\n",
    "\n",
    "from analyst_toolkit.m00_utils.config_loader import load_config\n",
    "\n",
    "# Path to master config (modify if needed)\n",
    "config_path = \"config/run_toolkit_config.yaml\"\n",
    "\n",
    "# Load full configuration dictionary\n",
    "config = load_config(config_path)\n",
    "\n",
    "# Extract run-level settings\n",
    "run_id = config.get(\"run_id\", \"default_run\")\n",
    "notebook_mode = config.get(\"notebook\", True)\n",
    "\n",
    "print(f\"🔧 Config loaded | Run ID: {run_id} | Notebook Mode: {notebook_mode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0410eb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📥 Load Raw Data from CSV\n",
    "\n",
    "from analyst_toolkit.m00_utils.load_data import load_csv\n",
    "\n",
    "# Load input path from the global config (or override manually)\n",
    "input_path = config.get(\"pipeline_entry_path\", \"data/raw/synthetic_penguins_v3.5.csv\")\n",
    "print(f\"📂 Loading data from: {input_path}\")\n",
    "\n",
    "# Load into DataFrame\n",
    "df_raw = load_csv(input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcff5859",
   "metadata": {},
   "source": [
    "### 🧪 Step 1: Run Initial Diagnostics (M01)\n",
    "\n",
    "This module generates a profile of the raw data: shape, types, nulls, skewness, and sample rows.\n",
    "\n",
    "This module profiles the raw dataset for key structural and quality checks:\n",
    "- **Memory, Shape, Dtypes**  \n",
    "- **Missing Values & Skewness**\n",
    "- **Duplicate Detection**\n",
    "- **Sample Rows & Descriptive Stats**\n",
    "\n",
    "✅ All results are rendered in a collapsible dashboard with exportable reports.  \n",
    "You can toggle inline previews and export settings via the YAML config (`diag_config_template.yaml`).\n",
    "\n",
    "\n",
    ">🛠️ To modify thresholds or toggle sections, edit the config under `diagnostics.settings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0893966a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 M01: Data Diagnostics – Profile Structure & Shape\n",
    "\n",
    "from analyst_toolkit.m00_utils.config_loader import load_config\n",
    "from analyst_toolkit.m00_utils.load_data import load_csv\n",
    "from analyst_toolkit.m01_diagnostics.run_diag_pipeline import run_diag_pipeline\n",
    "\n",
    "# --- Load module-specific config ---\n",
    "diag_config = load_config(\"config/diag_config_template.yaml\")\n",
    "diag_cfg = diag_config.get(\"diagnostics\", {})\n",
    "notebook_mode = diag_config.get(\"notebook\", True)\n",
    "run_id = diag_config.get(\"run_id\", \"demo_run\")\n",
    "\n",
    "# --- Load raw data from path defined in config ---\n",
    "input_path = diag_cfg.get(\"input_path\")\n",
    "if not input_path:\n",
    "    raise ValueError(\"🛑 No input_path specified in diagnostics config.\")\n",
    "df_raw = load_csv(input_path)\n",
    "\n",
    "# --- Run Diagnostics Module ---\n",
    "df_profiled = run_diag_pipeline(\n",
    "    config=diag_cfg,\n",
    "    df=df_raw,\n",
    "    notebook=notebook_mode,\n",
    "    run_id=run_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3f3e6b",
   "metadata": {},
   "source": [
    "### 🛡️ Step 2: Run Schema & Content Validation (M02)\n",
    "\n",
    "This module audits the dataset against a defined schema to catch issues early and guide cleaning steps:\n",
    "- **Expected Columns & Dtypes**  \n",
    "- **Allowed Categorical Values**\n",
    "- **Numeric Range Checks**\n",
    "- **Null Allowance (optional)**\n",
    "\n",
    "✅ All results are displayed in a styled validation dashboard with exportable reports.  \n",
    "You can define strict or flexible rules in the YAML config (`validation_config_template.yaml`).\n",
    "\n",
    "> 🛠️ To adjust enforcement (e.g. halt-on-fail), set `fail_on_error` and update rules under `validation.schema_validation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50d1299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🛡️ M02: Schema & Content Validation – First Audit Pass\n",
    "\n",
    "from analyst_toolkit.m00_utils.config_loader import load_config\n",
    "from analyst_toolkit.m00_utils.load_data import load_csv\n",
    "from analyst_toolkit.m02_validation.run_validation_pipeline import run_validation_pipeline\n",
    "\n",
    "# --- Load config and unpack validation settings ---\n",
    "val_config = load_config(\"config/validation_config_template.yaml\")\n",
    "val_cfg = val_config.get(\"validation\", {})\n",
    "notebook_mode = val_config.get(\"notebook\", True)\n",
    "run_id = val_config.get(\"run_id\", \"demo_run\")\n",
    "\n",
    "# --- Run Validation Module ---\n",
    "df_validated = run_validation_pipeline(\n",
    "    config=val_cfg,\n",
    "    df=df_profiled,\n",
    "    notebook=notebook_mode,\n",
    "    run_id=run_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b318304e",
   "metadata": {},
   "source": [
    "### 🧹 Step 3: Normalize & Standardize Data (M03)\n",
    "\n",
    "This module performs rule-based cleaning and normalization to prepare the dataset for certification:\n",
    "- **Column Renaming & Type Coercion**\n",
    "- **Value Mapping & Text Cleaning**\n",
    "- **Fuzzy Matching & Datetime Parsing**\n",
    "\n",
    "✅ Results are rendered in a structured dashboard with before/after comparisons and audit previews.  \n",
    "All rules and output paths are controlled via the YAML config (`normalization_config_template.yaml`).\n",
    "\n",
    "> 🛠️ To adjust cleaning logic, modify the `rules` block (e.g. `value_mappings`, `preview_columns`, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e192f2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧹 M03: Data Normalization – Standardizing Key Fields\n",
    "\n",
    "from analyst_toolkit.m00_utils.config_loader import load_config\n",
    "from analyst_toolkit.m03_normalization.run_normalization_pipeline import run_normalization_pipeline\n",
    "import logging\n",
    "\n",
    "# --- Load Config ---\n",
    "config = load_config(\"config/normalization_config_template.yaml\")\n",
    "norm_cfg = config.get(\"normalization\", {})\n",
    "run_id = config.get(\"run_id\")\n",
    "notebook_mode = config.get(\"notebook\", True)\n",
    "\n",
    "df_normalized = run_normalization_pipeline(\n",
    "    config=norm_cfg,\n",
    "    df=df_validated,\n",
    "    notebook=notebook_mode,\n",
    "    run_id=run_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27849087",
   "metadata": {},
   "source": [
    "### 🛡️ Step 4: Certification Gatekeeper (M04)\n",
    "\n",
    "This module enforces **strict schema and content rules** and is designed to **halt the pipeline** if violations are found:\n",
    "- ✅ All column names, data types, categorical values, and numeric ranges must pass\n",
    "- 🛑 **`fail_on_error: true`** triggers a hard stop on validation failure\n",
    "\n",
    "📦 This step can be run **at any point in the pipeline** — not just the end.  \n",
    "Use it wherever you want to **certify a dataset snapshot** or block further execution unless data meets expectations.\n",
    "\n",
    "✅ Results are rendered inline with full export support.  \n",
    "All certification rules live in the YAML config (`certification_config_template.yaml`).\n",
    "\n",
    "> 🛠️ Adjust gatekeeping behavior by modifying schema rules or toggling `fail_on_error`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3934847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🛡️ M02: Final Certification (Strict Validation Gatekeeper)\n",
    "\n",
    "import logging\n",
    "from analyst_toolkit.m00_utils.config_loader import load_config\n",
    "from analyst_toolkit.m02_validation.run_validation_pipeline import run_validation_pipeline\n",
    "\n",
    "# --- Load Certification Config ---\n",
    "config = load_config(\"config/certification_config_template.yaml\")\n",
    "cert_cfg = config.get(\"validation\", {})\n",
    "notebook_mode = config.get(\"notebook_mode\", True)\n",
    "run_id = config.get(\"run_id\")\n",
    "\n",
    "# --- Run Final Certification Pass ---\n",
    "logging.info(\"🚀 Starting M04: Certification (Validation Gatekeeper)\")\n",
    "\n",
    "df_certified = run_validation_pipeline(\n",
    "    config=cert_cfg,\n",
    "    notebook=notebook_mode,\n",
    "    df=df_normalized,\n",
    "    run_id=run_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7a02bb",
   "metadata": {},
   "source": [
    "### 🧹 Step 5: Deduplication (M05)\n",
    "\n",
    "This module identifies and handles **duplicate rows** in the dataset.\n",
    "\n",
    "You can choose to:\n",
    "- 🔍 **Flag duplicates** for review  \n",
    "- ✂️ **Remove duplicates** directly (default: keep first occurrence)\n",
    "\n",
    "✅ Configurable logic lets you define:\n",
    "- Which columns to check for duplication (`subset_columns`)\n",
    "- Whether to flag or drop (`mode: \"flag\"` or `\"remove\"`)\n",
    "- Columns to preview (hide IDs, timestamps, etc.)\n",
    "\n",
    "📄 Results are displayed with an inline preview and summary plots.\n",
    "\n",
    "> 🛠️ Adjust deduplication behavior in `dups_config_template.yaml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709014f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ♻️ D04: Dedupliction and Duplicates Handling\n",
    "\n",
    "from analyst_toolkit.m00_utils.config_loader import load_config\n",
    "from analyst_toolkit.m00_utils.load_data import load_csv\n",
    "from analyst_toolkit.m04_duplicates.run_dupes_pipeline import run_duplicates_pipeline\n",
    "import logging\n",
    "\n",
    "# --- Load Full Config ---\n",
    "config = load_config(\"config/dups_config_template.yaml\")\n",
    "dup_cfg = config.get(\"duplicates\", {})\n",
    "notebook_mode = config.get(\"notebook\", True)\n",
    "run_id = config.get(\"run_id\")\n",
    "\n",
    "\n",
    "\n",
    "# --- Run Duplicates Module ---\n",
    "df_deduped = run_duplicates_pipeline(\n",
    "    config=dup_cfg,\n",
    "    df=df_certified,\n",
    "    notebook=notebook_mode,\n",
    "    run_id=run_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d538dc27",
   "metadata": {},
   "source": [
    "### 📏 Step 6: Detect Outliers (M06)\n",
    "\n",
    "This module scans numeric columns for outliers using configurable logic:\n",
    "- **Z-Score** or **IQR** methods (per column or global default)\n",
    "- Adds binary flags (e.g., `*_outlier`) to the dataset if `append_flags: true`\n",
    "- Skips non-numeric or excluded fields via `exclude_columns`\n",
    "\n",
    "📊 **Interactive PlotViewer**  \n",
    "If enabled, the `PlotViewer` renders **boxplots, histograms, and violin plots** inline  \n",
    "— giving a fast visual summary of where anomalies occur.\n",
    "\n",
    "📁 **What’s Exported:**\n",
    "- ✅ `df_outliers_flagged`: DataFrame with new `_outlier` columns\n",
    "- ✅ `detection_results`: thresholds and summary tables\n",
    "- ✅ Plots: saved to `exports/plots/outliers/{run_id}/`\n",
    "- ✅ Report: XLSX or CSV, based on config\n",
    "\n",
    "> 🛠️ Configure methods, thresholds, excluded columns, and plot types in `outlier_config_template.yaml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5a5452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📏 M05: Detect Outliers and Plot Visuals\n",
    "\n",
    "import logging\n",
    "from analyst_toolkit.m00_utils.config_loader import load_config\n",
    "from analyst_toolkit.m05_detect_outliers.run_detection_pipeline import run_outlier_detection_pipeline\n",
    "import logging\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "config = load_config(\"config/outlier_config_template.yaml\")\n",
    "outlier_cfg = config.get(\"outlier_detection\", {})\n",
    "\n",
    "# Get global settings from the top level of the config\n",
    "notebook_mode = config.get(\"notebook\", True)\n",
    "run_id = config.get(\"run_id\") # Provide a default run_id\n",
    "\n",
    "# The 'df_deduped' variable should be the output from your M03 Duplicates module\n",
    "if 'df_deduped' in locals():\n",
    "    df_outliers_flagged, detection_results = run_outlier_detection_pipeline(\n",
    "        config=outlier_cfg,\n",
    "        df=df_deduped, \n",
    "        notebook=notebook_mode,\n",
    "        run_id=run_id\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c11ca0",
   "metadata": {},
   "source": [
    "### 🧼 Step 7: Handle Outliers (M07)\n",
    "\n",
    "This module applies cleanup strategies to flagged outliers from the detection step:\n",
    "- Strategies include:\n",
    "  - `'clip'`: Caps values to threshold bounds\n",
    "  - `'median'`: Imputes using median\n",
    "  - `'constant'`: Replaces with fixed value (e.g., `-999`)\n",
    "  - `'none'`: Leaves values untouched (default)\n",
    "\n",
    "⚙️ Strategy is configured per column or globally via `__default__` and `__global__`.\n",
    "\n",
    "📁 **What’s Exported:**\n",
    "- ✅ Cleaned DataFrame: `df_handled`\n",
    "- ✅ Handling report (XLSX/CSV)\n",
    "- ✅ Optional checkpoint joblib\n",
    "\n",
    "> 🛠️ Adjust cleanup logic, output paths, or constant fill values in `handling_config_template.yaml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e38936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧼 M06: Handle Outliers\n",
    "\n",
    "import logging\n",
    "from analyst_toolkit.m00_utils.config_loader import load_config\n",
    "from analyst_toolkit.m06_outlier_handling.run_handling_pipeline import run_outlier_handling_pipeline\n",
    "\n",
    "\n",
    "# --- Load Outlier Handling Config ---\n",
    "config = load_config(\"config/handling_config_template.yaml\")\n",
    "handling_cfg = config.get(\"outlier_handling\", {})\n",
    "run_id = config.get(\"run_id\")\n",
    "notebook_mode = config.get(\"notebook\", True)\n",
    "\n",
    "# Pass the entire detection_results dictionary, not its unpacked components.\n",
    "df_handled = run_outlier_handling_pipeline(\n",
    "    config=handling_cfg,\n",
    "    df=df_outliers_flagged,\n",
    "    detection_results=detection_results, # Pass the whole dictionary here\n",
    "    notebook=notebook_mode,\n",
    "    run_id=run_id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e3f0d1",
   "metadata": {},
   "source": [
    "### 🔧 Step 8: Impute Missing Values (M08)\n",
    "\n",
    "This module fills missing (`NaN`) values using a column-specific strategy:\n",
    "- `'mean'`, `'median'`, or `'mode'` for numeric/categorical inference\n",
    "- `'constant'` for fixed fallback values (e.g., `\"UNKNOWN\"` or `\"1900-01-01\"`)\n",
    "- Strategy is configured via the `rules.strategies` section in the YAML\n",
    "\n",
    "📊 If enabled, comparison plots show how categorical columns changed post-imputation  \n",
    "(using the same PlotViewer system).\n",
    "\n",
    "📁 **What’s Exported:**\n",
    "- ✅ Imputed DataFrame: `df_imputed`\n",
    "- ✅ Report: imputation log (XLSX/CSV)\n",
    "- ✅ Plots: before/after comparisons (if enabled)\n",
    "- ✅ Optional checkpoint joblib\n",
    "\n",
    "> 🛠️ Configure logic and column-specific strategies in `imputation_config_template.yaml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f059ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#🔧 M07: Impute Data and Plot Summary Visuals\n",
    "\n",
    "import logging\n",
    "from analyst_toolkit.m00_utils.config_loader import load_config\n",
    "from analyst_toolkit.m07_imputation.run_imputation_pipeline import run_imputation_pipeline\n",
    "\n",
    "# Load the configuration for the imputation module\n",
    "config = load_config(\"config/imputation_config_template.yaml\") \n",
    "imputation_cfg = config.get(\"imputation\", {})\n",
    "run_id = config.get(\"run_id\")\n",
    "notebook_mode = config.get(\"notebook\", True)\n",
    "\n",
    "df_imputed = run_imputation_pipeline(\n",
    "    config=imputation_cfg,\n",
    "    notebook=notebook_mode,\n",
    "    df=df_handled,  # Pass the existing DataFrame here\n",
    "    run_id=run_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374dfb56",
   "metadata": {},
   "source": [
    "### 🧩 Behind the Scenes: Utility & Visual Modules\n",
    "\n",
    "Several specialized support modules power the Analyst Toolkit pipeline behind the scenes.  \n",
    "These are not called directly in the notebook, but are crucial to the system’s flexibility and polish:\n",
    "\n",
    "#### 🧰 `m00_utils/`\n",
    "- `config_loader.py`: Robust loader with support for environment paths and nested YAMLs\n",
    "- `load_data.py`: Abstracted CSV/Joblib loader with encoding fallback\n",
    "- `export_utils.py`: Modular export system for saving reports and checkpoints\n",
    "- `rendering_utils.py`: Styled HTML table generator for dashboard outputs\n",
    "\n",
    "#### 📊 `m08_visuals/`\n",
    "- `distributions.py`: Boxplots, histograms, and violin plots for outlier detection\n",
    "- `summary_plots.py`: Heatmaps, missingness matrices, and dtype summaries\n",
    "- `plot_viewer.py`: Interactive PlotViewer widget for inspecting flagged values and category shifts\n",
    "\n",
    "> ⚙️ These modules enable notebook-mode display, CLI compatibility, YAML-driven plotting, and clean HTML export dashboards.\n",
    "\n",
    "📁 Explore these utilities in the `/src/` directory to understand how the toolkit remains modular, extensible, and production-grade."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a2b8fa",
   "metadata": {},
   "source": [
    "## 🎬 Step 9: Final Auditing and Certifaction (M10)\n",
    "\n",
    "This final module performs a comprehensive audit of the cleaned dataset and applies strict quality checks before certification.\n",
    "\n",
    "It serves as the **final quality gate** and includes:\n",
    "- ✅ **Final Edits:** Drop or rename columns, coerce dtypes as needed\n",
    "- ✅ **Certification Check:** Applies validation rules with `fail_on_error: true` to enforce schema, dtypes, and content requirements\n",
    "- ✅ **Lifecycle Comparison:** Compares raw vs final structure, nulls, and column presence\n",
    "- ✅ **Capstone Report:** Renders a complete dashboard summarizing pipeline impact and status\n",
    "\n",
    "🛡️ If any rule is violated (e.g., unexpected nulls or schema mismatch), the system halts and logs failure details for debugging.\n",
    "\n",
    "📁 **What’s Exported:**\n",
    "- Final Audit Report (XLSX and Joblib)\n",
    "- Final Certified Dataset (CSV and Joblib)\n",
    "- Inline dashboard with all results\n",
    "\n",
    "> 🛠️ Customize certification rules, null restrictions, or output paths in `final_audit_config_template.yaml`.\n",
    "\n",
    "🎉 Once this step passes, your dataset is ready for **production use or modeling pipelines**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e51d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎬 M10: Final Auditing and Certifaction \n",
    "\n",
    "from analyst_toolkit.m10_final_audit.final_audit_pipeline import run_final_audit_pipeline\n",
    "from analyst_toolkit.m00_utils.config_loader import load_config\n",
    "\n",
    "# --- Load Config ---\n",
    "config_path = \"config/final_audit_config_template.yaml\"\n",
    "config = load_config(config_path)\n",
    "\n",
    "# --- Extract settings (this part is correct) ---\n",
    "notebook_mode = config.get(\"notebook\", True)\n",
    "run_id = config.get(\"run_id\")\n",
    "\n",
    "# --- Run Final Audit ---\n",
    "# The run_id is handled internally by the pipeline function from the config.\n",
    "df_final_clean = run_final_audit_pipeline(\n",
    "    config=config,\n",
    "    df=df_imputed,  # This correctly passes the processed DataFrame\n",
    "    notebook=notebook_mode,\n",
    "    run_id=run_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa9f35c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🧭 What’s Next?\n",
    "\n",
    "Congratulations — you’ve now completed a full walkthrough of the Analyst Toolkit pipeline using synthetic Palmer Penguins data!\n",
    "\n",
    "Here are some suggested next steps:\n",
    "\n",
    "1. 🔍 **Explore Outputs**  \n",
    "   - Review the exported reports and plots in the `exports/` folder\n",
    "   - Inspect final audit and certification summaries\n",
    "\n",
    "2. 🧪 **Test with Other Datasets**  \n",
    "   - Replace the penguin dataset with your own CSV in the YAML configs\n",
    "   - Adjust schema, value, and range rules accordingly\n",
    "\n",
    "3. 📓 **Use the Full Pipeline Script**  \n",
    "   - Try running `run_toolkit_pipeline.py` in CLI or notebook mode for a full end-to-end execution\n",
    "   - Config: `config/run_toolkit_config.yaml`\n",
    "\n",
    "4. 🛠️ **Customize Modules**  \n",
    "   - Add new modules (e.g., feature engineering, modeling)\n",
    "   - Use your own diagnostic thresholds or imputation logic\n",
    "\n",
    "5. 🚀 **Package or Deploy**  \n",
    "   - Deploy the toolkit in production (Airflow, Papermill, GitHub Actions, etc.)\n",
    "   - Or package it as a Python module for reuse\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analyst_toolkit_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
