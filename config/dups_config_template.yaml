# ------------------------------------------------------------------------------
# 📄 Config File: run_duplicates_config.yaml
# 🧩 Module: Duplicates (M04)
# 📌 Purpose: Detects and handles duplicate rows based on configured logic.
# ------------------------------------------------------------------------------
# This module can flag or remove duplicates depending on the `mode` setting.
# ------------------------------------------------------------------------------

# 📥 Execution context
notebook: true
run_id: "removal_run"
logging: "auto"             # Options: 'on', 'off', or 'auto'

# 🔎 Duplicate detection settings
duplicates:
  run: true

  # 🎯 Target subset (null = use all columns)
  subset_columns: ['tag_id', 'species', 'capture_date']

  # 🧭 Deduplication logic
  keep: 'first'          # Options: 'first', 'last', or False (to drop all duplicates)
  mode: 'remove'           # Options: 'remove', 'flag'

  # 📂 Input source
  input_path: "exports/joblib/{run_id}/{run_id}_m02_certified.joblib"

  # ⚙️ Operational settings (required by pipeline)
  settings:
    checkpoint:
      run: true
      checkpoint_path: "exports/joblib/{run_id}/{run_id}_m04__dupes_checkpoint.joblib"

    export: true
    export_path: "exports/reports/duplicates/duplicates_report.xlsx"
    as_csv: false          # false = single XLSX report with multiple sheets

    show_inline: true

    plotting:
      run: true
      save_dir: "exports/plots/duplicates/"

  # 🧼 Optional preview cleanup for schema-variant files
  preview_drop_columns:
    - "timestamp"
    - "script_name"
    - "user"