<p align="center">
  <img src="repo_files/analyst_toolkit_banner.png" alt="Analyst Toolkit Logo" width="1000"/>
  <br>
  <em>Self-Healing Data Audit &nbsp;Â·&nbsp; Data QA + Cleaning Engine &nbsp;Â·&nbsp; MCP Server</em>
</p>
<p align="center">
  <img alt="MIT License" src="https://img.shields.io/badge/license-MIT-blue">
  <img alt="Status" src="https://img.shields.io/badge/status-stable-brightgreen">
  <img alt="Version" src="https://img.shields.io/badge/version-v0.4.2-blueviolet">
  <a href="https://github.com/G-Schumacher44/analyst_toolkit/actions/workflows/analyst-toolkit-mcp-ci.yml">
    <img alt="CI" src="https://github.com/G-Schumacher44/analyst_toolkit/actions/workflows/analyst-toolkit-mcp-ci.yml/badge.svg">
  </a>
  <img alt="GHCR" src="https://img.shields.io/badge/ghcr.io-analyst--toolkit--mcp-blue?logo=docker">
</p>

# ğŸ§ª Analyst Toolkit

Modular data QA and preprocessing toolkit â€” run as a Jupyter notebook pipeline, CLI, or MCP server with Docker and GCS support.

## ğŸ†• Version 0.4.2: The "Self-Healing" Audit

This major update transforms the toolkit from a collection of utilities into a cohesive, autonomous auditing engine.

1. **Listen (Inference):** Predict data needs automatically using `infer_configs`.
2. **Diagnose (Validation):** Detect holes (nulls) and bumps (outliers) with a single score.
3. **Heal (Auto-Apply):** Automatically repair data based on inferred rules using `auto_heal`.
4. **Certify (Audit):** Generate a tamper-proof health report and sequence ledger.

---

## ğŸ‘€ MCP Ecosystem

Ship the toolkit as an MCP server and plug it into Claude Desktop, FridAI, or any JSON-RPC 2.0 client.

- **â›“ï¸ Pipeline Mode:** Chain multiple tools in memory using `session_id` â€” no intermediate saves.
- **ğŸ•¹ï¸ Executive Cockpit:** Get a **0-100 Data Health Score** and a detailed **Healing Ledger**.
- **ğŸ“€ Golden Templates:** Example templates tuned for typical fraud/migration/compliance patterns.
- **ğŸ“š Template Resources:** MCP `resources/list` + `resources/read` expose standard and golden YAML templates directly to clients/agents.
- **ğŸ¤– Auto-Heal:** One-click inference and repair â€” from raw data to certified output in a single tool call.
- [ğŸ“¡ MCP Server Guide](resource_hub/mcp_server_guide.md) â€” full setup, tool reference, and host integrations

---

## TL;DR

- Modular execution by stage (diagnostics, validation, normalization, etc.)
- Inline dashboards and exportable HTML + Excel reports
- Full pipeline execution (notebook or CLI)
- YAML-configurable logic per module
- Checkpointing and joblib persistence
- MCP server â€” expose all toolkit modules as tools to any MCP-compatible host
- ğŸ§ Built using synthetic data from the [dirty_birds_data_generator](https://github.com/G-Schumacher44/dirty_birds_data_generator)
- ğŸ“‚ [Sample output](exports/sample/) (plots, reports, cleaned dataset)

---

## ğŸ“ Resource Hub (Start Here)

- [ğŸ“¡ MCP Server Guide](resource_hub/mcp_server_guide.md) â€” Setup, tool reference, FridAI + Claude Desktop integration
- [ğŸ§­ Config Guide](resource_hub/config_guide.md) â€” Overview of all YAML configuration files
- [ğŸ“¦ Config Templates](config/) â€” Full set of starter YAMLs for each module (in `config/`)
- [ğŸ“˜ Usage Guide](resource_hub/usage_guide.md) â€” Running the toolkit via notebooks or CLI
- [ğŸ“— Notebook Usage Guide](resource_hub/notebook_usage_guide.md) â€” Full breakdown of how each module is used in notebooks
- [ğŸ¤ Contributing Guide](CONTRIBUTING.md) â€” Development workflow, quality gates, and PR expectations
- [ğŸ“ Changelog](CHANGELOG.md) â€” Versioned, deterministic release notes

---

### ğŸ“š Quick Start Notebooks

<p align="left">
  <a href="notebooks/00_analyst_toolkit_modular_demo.ipynb" style="margin-right: 10px;">
    <img alt="Modular Demo" src="https://img.shields.io/badge/Demo%20Notebook-Modular-blue?style=for-the-badge&logo=jupyter" />
  </a>
  &nbsp;&nbsp;
  <a href="notebooks/01_analyst_toolkit_pipeline_demo.ipynb">
    <img alt="Pipeline Demo" src="https://img.shields.io/badge/Demo%20Notebook-Full%20Pipeline-green?style=for-the-badge&logo=python" />
  </a>
</p>

---

## ğŸ“¸ Dashboard Snapshots

<div align="center">
  <table>
    <tr>
      <td><img src="repo_files/db_screen_00.png" alt="Diagnostics dashboard" width="400"/></td>
      <td><img src="repo_files/db_screen_1.png" alt="Validation dashboard" width="400"/></td>
    </tr>
    <tr>
      <td><img src="repo_files/db_screen_2.png" alt="Outlier detection dashboard" width="400"/></td>
      <td><img src="repo_files/db_screen_3.png" alt="Imputation dashboard" width="400"/></td>
    </tr>
  </table>
</div>

<p align="center"><em>Inline dashboards rendered per-module â€” diagnostics, validation, outlier detection, and imputation.</em></p>

---

## ğŸ§° Installation

**ğŸ”§ Local Development**

```bash
git clone https://github.com/G-Schumacher44/analyst_toolkit.git
cd analyst_toolkit
make install-dev       # editable install + pre-commit hooks
```

**With MCP server deps**

```bash
pip install "analyst_toolkit[mcp] @ git+https://github.com/G-Schumacher44/analyst_toolkit.git"
```

**With notebook extras**

```bash
pip install "analyst_toolkit[notebook] @ git+https://github.com/G-Schumacher44/analyst_toolkit.git"
```

**Install from GitHub (bare)**

```bash
pip install git+https://github.com/G-Schumacher44/analyst_toolkit.git
```

---

## ğŸ¤– MCP Server

The toolkit ships with a built-in MCP server that exposes every module as a tool callable by any MCP-compatible host â€” Claude Desktop, FridAI, VS Code, or any JSON-RPC 2.0 client.

**Pull from GHCR:**

```bash
docker pull ghcr.io/g-schumacher44/analyst-toolkit-mcp:latest
```

**Or build and start locally:**

```bash
make mcp-up        # docker-compose up --build -d
make mcp-health    # curl /health and pretty-print response
make mcp-logs      # tail logs
make mcp-down      # stop
# extra runtime checks:
curl http://localhost:8001/ready | python3 -m json.tool
curl http://localhost:8001/metrics | python3 -m json.tool
```

**Call a tool:**

```bash
curl -X POST http://localhost:8001/rpc \
  -H "Content-Type: application/json" \
  -d '{"jsonrpc":"2.0","id":1,"method":"tools/call","params":{"name":"outliers","arguments":{"gcs_path":"gs://my-bucket/data/"}}}'
```

Tools accept a `gcs_path` (GCS URI, local `.parquet`, or local `.csv`) and an optional `config` dict matching the module's YAML structure. HTML reports are generated automatically when `ANALYST_REPORT_BUCKET` is set, or explicitly with `export_html: true` in the config.
If template/resource reads are timing out under load, tune `ANALYST_MCP_RESOURCE_TIMEOUT_SEC` and `ANALYST_MCP_TEMPLATE_IO_TIMEOUT_SEC`.
For structured request lifecycle logs, set `ANALYST_MCP_STRUCTURED_LOGS=true`.
For token auth in networked deployments, set `ANALYST_MCP_AUTH_TOKEN` and send `Authorization: Bearer <token>`.

> See [ğŸ“¡ MCP Server Guide](resource_hub/mcp_server_guide.md) for full setup, tool reference, FridAI integration, Claude Desktop wiring, and environment variable reference.

---

## ğŸ§¾ Configuration

Each module is controlled by a YAML file stored in `config/`.

Example:

```yaml
validation:
  input_path: "data/raw/synthetic_penguins_v3.5.csv"
  schema_validation:
    run: true
    rules:
      expected_columns: [...]
```

For full structure and explanation, [ğŸ“˜ Read the Full Configuration Guide](resource_hub/config_guide.md)

---

## ğŸ§ª Usage

<details>
<summary>ğŸ““ Notebook Use (Modular)</summary>

Run each module interactively inside a Jupyter notebook.

**Example**

```python
from analyst_toolkit.m02_validation.run_validation_pipeline import run_validation_pipeline
from analyst_toolkit.m00_utils.config_loader import load_config
from analyst_toolkit.m00_utils.load_data import load_csv

# --- Load config and data ---
config = load_config("config/validation_config_template.yaml")
df = load_csv("path/to/your/data.csv")

# --- Extract global settings ---
notebook_mode = config.get("notebook", True)
run_id = config.get("run_id", "demo_run")

# --- Run Validation Module ---
df_validated = run_validation_pipeline(
    config=config,
    df=df,
    notebook=notebook_mode,
    run_id=run_id
)
```

Modules render dashboards inline if `notebook: true` is set in the YAML config.

> See [ğŸ“— Notebook Usage Guide](resource_hub/notebook_usage_guide.md) for a full breakdown

</details>

<details>
<summary>ğŸ““ Notebook Use (Full Pipeline)</summary>

Run the full pipeline interactively inside a Jupyter notebook.

**Example**

```python
from analyst_toolkit.run_toolkit_pipeline import run_full_pipeline

final_df = run_full_pipeline(config_path="config/run_toolkit_config.yaml")
```

Each module reads its own YAML config file, with optional global overrides in `config/run_toolkit_config.yaml`. Example:

```yaml
# --- Global Run Settings ---
run_id: "CLI_2_QA"
notebook: false

# --- Pipeline Entry Point ---
pipeline_entry_path: "data/raw/synthetic_penguins_v3.5.csv"

modules:
  diagnostics:
    run: true
    config_path: "config/diag_config_template.yaml"

  validation:
    run: true
    config_path: "config/validation_config_template.yaml"
```

> See [ğŸ“— Notebook Usage Guide](resource_hub/notebook_usage_guide.md) for a full breakdown

</details>

<details>
<summary>ğŸ” Full Pipeline (CLI)</summary>

```bash
make pipeline                              # uses config/run_toolkit_config.yaml
make pipeline CONFIG=config/my_config.yaml # custom config
# or directly:
python -m analyst_toolkit.run_toolkit_pipeline --config config/run_toolkit_config.yaml
```

> For full structure and explanation, [ğŸ“˜ Read the Full Usage Guide](resource_hub/usage_guide.md)

</details>

---

<details>
<summary><strong>ğŸ“ Notes from the Dev</strong></summary>
<br>

**Why build a toolkit for analysts?**

I built the Analyst Toolkit to eliminate the most frustrating part of the analytics workflow â€” wasting hours on boilerplate cleaning when we should be exploring, validating, and learning. This system gives you:

- A one-stop first-pass QA and cleaning run, fully executable in a single notebook
- Total modularity â€” run stage by stage or all at once
- YAML-driven control over everything from null handling to audit thresholds

Every step leaves behind artifacts: dashboards, exports, warnings, checkpoints. You don't just *run* the pipeline â€” you *see* it working. You know what changed, where it changed, and what the implications are downstream. This is **auditable automation** â€” the insights are always there when you need them.

It is overbuilt in the ways that matter: transparency, reproducibility, trust. It's designed for team collaboration, for portfolio projects, for production QA. It's for your current self â€” and your future self â€” when you need to revisit a workflow six months from now.

The system is human readable and YAML-driven â€” for your team, your stakeholders, and yourself.

</details>

<details>
<summary><strong>ğŸ§ Dirty Birds: Palmer Penguins Synthetic Dataset v3.5</strong></summary>
<br>

This toolkit is developed and tested using the **Dirty Birds v3.5** dataset â€” a fully synthetic recreation of the Palmer Penguins dataset, purposefully enriched with ambiguity, anomalies, and missing data. The dataset is generated using <a href="https://github.com/G-Schumacher44/dirty_birds_data_generator">penguin_synthetic_data_generator.py</a>, a synthetic data generator that simulates viable research data and injects realistic biological variance and field collection noise for robust QA testing.

ğŸ§ Features include:
- Categorical anomalies (typos, whitespace, & swaps)
- Numeric outliers and skew (both in error and in biological boundaries)
- Nullable fields in both wide and narrow formats
- Simulated noise to match real-world field data collection

</details>

<details>
<summary><strong>ğŸ«† Version Release Notes</strong></summary>

**v0.4.0 â€” The Cockpit Upgrade**
- **State Management:** Introduced `StateStore` for in-memory DataFrame persistence between tool calls via `session_id`.
- **Data Health Score:** Every run now generates a weighted 0-100 score (Completeness, Validity, Uniqueness, Consistency).
- **Healing Ledger:** Persistent JSON/GCS history tracking every transformation made during a run.
- **Golden Templates:** Example templates tuned for typical fraud/migration/compliance patterns (bundled in the image under `config/golden_templates/`).
- **Autonomous Tools:** Added `auto_heal` (one-click cleaning) and `drift_detection` (schema/statistical comparison).
- **Configuration Intelligence:** Added `get_config_schema` to return JSON Schemas for every module.

**v0.3.0**
- **MCP Server:** New `analyst_toolkit/mcp_server/` package exposes all toolkit modules as MCP tools over JSON-RPC 2.0 (HTTP `/rpc`) and stdio transport.
- **HTML Reports:** All modules can emit self-contained single-page HTML reports.
- **Docker / GHCR:** Image published to `ghcr.io/g-schumacher44/analyst-toolkit-mcp` on every push to main.
- **CI + Quality:** GitHub Actions: ruff lint, mypy, pytest, Docker build + GHCR push on main.

**v0.2.1**
- **Normalization Â· Datetime parsing:** Multi-format support, strict mode, `dayfirst`/`yearfirst`/`utc` options.
- **Exports Â· Excel date stability:** Explicit date formats for cross-platform rendering.
- **Duplicates Â· Subset-focused clusters:** Dashboard now focuses on chosen `subset_columns` for clarity.

**v0.2.0**
- **Standardized Configuration Handling:** All modules now intelligently parse their own configuration blocks.
- **Simplified Module API:** Runners accept the full config object â€” no manual unpacking needed.

**v0.1.3**
- Refactored Duplicates Module (M04) with correct flag/remove modes and decoupled detection logic.

**v0.1.2**
- Core module scaffolding complete (M01â€“M10), full pipeline execution, inline dashboards, joblib checkpointing.

</details>

<details>
<summary>ğŸ“‚ Project Structure</summary>

```
ğŸ“¦ src/                                    # Source root
â”‚
â”œâ”€â”€ analyst_toolkit/                       # ğŸ”§ Main toolkit package
â”‚   â”œâ”€â”€ run_toolkit_pipeline.py            # CLI + notebook entrypoint
â”‚   â”‚
â”‚   â”œâ”€â”€ m00_utils/                         # Shared utilities
â”‚   â”‚   â”œâ”€â”€ config_loader.py               # YAML config loading and merging
â”‚   â”‚   â”œâ”€â”€ load_data.py                   # CSV/parquet ingestion
â”‚   â”‚   â”œâ”€â”€ export_utils.py                # Excel + HTML export helpers
â”‚   â”‚   â”œâ”€â”€ report_generator.py            # Self-contained HTML report builder
â”‚   â”‚   â”œâ”€â”€ scoring.py                     # Data health scoring (0-100)
â”‚   â”‚   â”œâ”€â”€ rendering_utils.py             # Shared display/rendering helpers
â”‚   â”‚   â”œâ”€â”€ data_viewer.py                 # DataFrame preview utilities
â”‚   â”‚   â”œâ”€â”€ plot_viewer.py                 # Inline plot display
â”‚   â”‚   â””â”€â”€ plot_viewer_comparison.py      # Before/after comparison plots
â”‚   â”‚
â”‚   â”œâ”€â”€ m01_diagnostics/                   # Data profiling and structural diagnostics
â”‚   â”œâ”€â”€ m02_validation/                    # Schema validation and certification gate
â”‚   â”œâ”€â”€ m03_normalization/                 # Data cleaning and standardization
â”‚   â”œâ”€â”€ m04_duplicates/                    # Duplicate detection and removal
â”‚   â”œâ”€â”€ m05_detect_outliers/               # Outlier detection (IQR, z-score)
â”‚   â”œâ”€â”€ m06_outlier_handling/              # Outlier imputation or transformation
â”‚   â”œâ”€â”€ m07_imputation/                    # Missing data imputation
â”‚   â”œâ”€â”€ m08_visuals/                       # Plotting utilities and dashboard rendering
â”‚   â”‚   â”œâ”€â”€ comparison_plots.py            # Before/after visual comparisons
â”‚   â”‚   â”œâ”€â”€ distributions.py               # Distribution and histogram plots
â”‚   â”‚   â””â”€â”€ summary_plots.py               # Summary/overview charts
â”‚   â”‚
â”‚   â”œâ”€â”€ m10_final_audit/                   # Final audit, edits, and pipeline certification
â”‚   â”‚
â”‚   â””â”€â”€ mcp_server/                        # MCP server â€” exposes toolkit as tools over JSON-RPC/stdio
â”‚       â”œâ”€â”€ server.py                      # FastAPI /rpc dispatcher + stdio transport
â”‚       â”œâ”€â”€ io.py                          # GCS/parquet/CSV data loading + report upload
â”‚       â”œâ”€â”€ config_models.py               # Pydantic models for typed config validation
â”‚       â”œâ”€â”€ schemas.py                     # TypedDicts and JSON Schema for tool I/O
â”‚       â”œâ”€â”€ registry.py                    # Tool self-registration and dispatch
â”‚       â”œâ”€â”€ state.py                       # StateStore â€” in-memory session management
â”‚       â”œâ”€â”€ templates.py                   # Golden template loader and resolver
â”‚       â””â”€â”€ tools/                         # Self-registering tool modules (one per toolkit module)
â”‚
â”œâ”€â”€ ğŸ§ª notebooks/                          # Interactive tutorial notebooks (modular & full run)
â”‚
â”œâ”€â”€ âš™ï¸ config/                             # YAML configuration files (one per module + full run)
â”‚   â””â”€â”€ golden_templates/                  # Best-practice configs for Fraud, Migration, Compliance
â”‚
â”œâ”€â”€ ğŸ“‚ data/
â”‚   â”œâ”€â”€ raw/                               # Original input datasets
â”‚   â”œâ”€â”€ processed/                         # Final certified outputs (.csv)
â”‚   â””â”€â”€ features/                          # Optional engineered features
â”‚
â”œâ”€â”€ ğŸ“¤ exports/
â”‚   â””â”€â”€ sample/                            # Sample media from a QA run
â”‚
â”œâ”€â”€ tests/                                 # Pytest test suite (MCP smoke, unit tests)
â”œâ”€â”€ resource_hub/                          # Reference, guidebooks, documentation
â”œâ”€â”€ Makefile                               # Common dev and ops commands
â”œâ”€â”€ pyproject.toml                         # Build config and optional extras
â”œâ”€â”€ environment.yaml                       # Conda environment definition
â”œâ”€â”€ requirements-mcp.txt                   # MCP server pip requirements
â”œâ”€â”€ Dockerfile.mcp                         # MCP server container
â””â”€â”€ docker-compose.mcp.yml                 # Docker Compose for local MCP server
```

</details>

---

## ğŸ¤ Contributing & Support

- [Contributing Guide](CONTRIBUTING.md) â€” setup, branch workflow, and quality gates
- [Security Policy](SECURITY.md) â€” responsible vulnerability disclosure process
- [Bug Report Template](.github/ISSUE_TEMPLATE/bug_report.md)
- [Feature Request Template](.github/ISSUE_TEMPLATE/feature_request.md)
- [Documentation Template](.github/ISSUE_TEMPLATE/documentation.md)
- [Pull Request Template](.github/PULL_REQUEST_TEMPLATE.md)

---

## ğŸ¤ On Generative AI Use

Generative AI tools (Gemini 2.5-PRO, ChatGPT 4o - 4.1, Claude Sonnet) were used throughout this project as part of an integrated workflow â€” supporting code generation, documentation refinement, and idea testing. These tools accelerated development, but the logic, structure, and documentation reflect intentional, human-led design. This repository reflects a collaborative process: where automation supports clarity, and iteration deepens understanding.

---

## ğŸ“¦ Licensing

This project is licensed under the [MIT License](LICENSE).
